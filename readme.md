# ğŸ¤– LLM Chatbot API (FastAPI + Hugging Face)

A **production-ready chatbot backend** built using **FastAPI**, **Hugging Face LLMs**, and **dynamic prompt templates**.
Designed with **clean architecture**, **service separation**, and **agile sprint-based development**, similar to how real-world AI teams build systems.

---

## ğŸ“Œ Project Overview

This project implements a **scalable chatbot API** that:

* Uses **Hugging Face LLMs** for text generation
* Supports **dynamic prompt templates**
* Is structured for **production deployment**
* Can be extended with **chat history, memory, embeddings, and RAG**

The goal is to build a **real AI backend**, not a demo script.

---

## ğŸ§  Key Features

* âœ… FastAPI-based REST API
* âœ… Modular & scalable folder structure
* âœ… Dynamic chat prompt templates
* âœ… Hugging Face model integration
* âœ… Clean service-layer architecture
* âœ… Swagger UI for API testing
* ğŸš§ Chat history & memory (Sprint 3)
* ğŸš§ RAG & vector database (future)

---

## ğŸ—ï¸ Tech Stack

| Layer           | Technology                       |
| --------------- | -------------------------------- |
| API Framework   | FastAPI                          |
| LLM             | Hugging Face                     |
| Prompting       | LangChain-style prompt templates |
| Server          | Uvicorn                          |
| Environment     | Python 3.10+                     |
| Version Control | Git + GitHub                     |

---

## ğŸ“‚ Project Structure

```
chatbot-api/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ chat.py          # Chat API endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ chat_service.py  # LLM interaction logic
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ chat_prompt.py  # Prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ chat.py         # Request & response models
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py       # Environment & settings
â”‚   â”‚
â”‚   â””â”€â”€ main.py             # FastAPI app entry point
â”‚
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run.sh / run.ps1
```

This structure follows **industry best practices** for AI backends.

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/chatbot-api.git
cd chatbot-api
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate it:

**Windows**

```bash
venv\Scripts\activate
```

**Mac/Linux**

```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure Environment Variables

Create a `.env` file:

```env
HUGGINGFACEHUB_API_TOKEN=your_hf_token_here
```

---

### 5ï¸âƒ£ Run the Server

```bash
uvicorn app.main:app --reload
```

Server will start at:

```
http://127.0.0.1:8000
```

---

## ğŸ“˜ API Documentation

Swagger UI:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ’¬ Chat API

### Endpoint

```
POST /api/v1/chat
```

### Request Body

```json
{
  "session_id": "abc123",
  "message": "What is LBW in cricket?"
}
```

### Response

```json
{
  "session_id": "abc123",
  "answer": "LBW stands for Leg Before Wicket..."
}
```

---

## ğŸ§© Prompt Design (Concept)

The chatbot uses **dynamic prompt templates**:

```
System: You are a helpful AI assistant.
User: {user_message}
Assistant:
```

This allows:

* Persona control
* Domain-specific behavior
* Easy prompt updates without touching API logic

---

## ğŸ§ª Sprint-Based Development

### âœ… Sprint 1 â€“ Project Foundation (Completed)

* Architecture design
* Tech stack selection
* Folder structure
* README & documentation

### ğŸš§ Sprint 2 â€“ Core Chat API (In Progress)

* FastAPI endpoints
* Hugging Face integration
* Dynamic prompt handling

### ğŸ”œ Sprint 3 â€“ Chat Memory

* Message history
* Session-based conversations

### ğŸ”œ Sprint 4 â€“ RAG & Embeddings

* Vector database
* Document-based Q&A

---

## ğŸ›¡ï¸ Production Readiness

Planned improvements:

* Centralized logging
* Rate limiting
* Async model calls
* Dockerization
* CI/CD pipeline
* API authentication

---

## ğŸ¯ Why This Project Matters

This project demonstrates:

* Real-world **AI backend design**
* **Prompt engineering** in production
* Clean separation of concerns
* Agile sprint methodology
* Readiness for **enterprise-scale AI systems**

Perfect for:

* AI Engineer roles
* Backend + LLM positions
* Startup or enterprise AI teams

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

**Kaustubh Dwivedi**
AI / Backend Engineer
ğŸš€ Building production-grade AI systems

